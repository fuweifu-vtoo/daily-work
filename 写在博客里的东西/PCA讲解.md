#从协方差矩阵到PCA再到矩阵分解
本文将要从最简单的方式记录PCA相关的知识内容，涵盖PCA的先验知识（协方差矩阵），PCA的原理（两种理解方式）和PCA的应用（图像压缩、数据降维、数据预处理）三个方面。
##协方差矩阵
我对协方差矩阵的理解最早出现于概率统计，当时有一个概念是协方差，之后推广到协方差矩阵，在协方差之前还有方差的概念，所以在这做一个总结：  

- 方差：  对于一个随机变量X而言,记作D(X)
- 协方差：  对于随机变量X和Y而言，X和Y的协方差记作cov(X,Y)
- 协方差矩阵：  用以时间为变量的随机过程举例，<a href="http://www.codecogs.com/eqnedit.php?latex={x_{a}}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?{x_{a}}" title="{x_{a}}" /></a>

之所以要讲协方差矩阵，是因为在PCA中要使用到。
##PCA的原理  
PCA的全称是primary component analysis，即主成分分析。它是一种被广泛应用的技术，应用领域包括降维、有损数据压缩、特征提取等。对PCA的原理的理解有两种方式，一种是最大方差形式，一种是最小误差形式，在这里只介绍第一种。
###pca的降维原理
我们在处理高维数据的时候，为了能降低后续计算的复杂度，在“预处理”阶段通常要先对原始数据进行降维，降维要遵循一个指导思想，那就是：找出最能够代表原始数据的投影方法。原始数据可以被认为是有用信号，而要被丢弃的数据便是噪声或者冗余。以下图为例：  
<center>
<img src="https://pic.superbed.cn/item/5c83a1bb3a213b04178fb338" width="50%"/></center>
这张图中的点都是二维数据，如果必须要降到一维，找到一个最能代表此组数据的方向，则图中蓝色的线的方向便是最佳方向，称为投影方向。如果使x轴或者Y轴当投影方向，可以发现五个点的间距会变得不明显，也就是方差不会很大。  
其实方差大的方向是信号的方向，方差小的方向是噪声的方向，在数字信号处理中，往往要提高信噪比。对上图来说，如果只保留信号方向的数据，便是对原数据做了不错的近似，也做到了降维处理。
##PCA中的谱分解和奇异值分解

###PCA中的谱分解

PCA有两种定义方式，两种定义会得到同样的算法。一种是最大化投影点的方差，称作最大方差形式，一种是最小化投影误差的平方和，称作最小误差形式。而协方差矩阵是第一种定义方式的关键。

考虑一组观测数据集{xn}，其中n = 1,......,N，因此是一个D维度欧几里得空间中的变量。倘若需要将数据降维至M维，其中M<D，同时最大化投影数据的方差，可以考虑以下方式。  

首先考虑一维空间。使用一个D维向量定义这个空间的方向，为了方便起见，这个向量取做单位向量，从而，而对的范数或者大小无要求。这样，一个数据点会被投影到一个标量值上。对这些标量取均值，得到。因为得到投影数据的方差如式所示。  

![](https://pic.superbed.cn/item/5c83a29d3a213b04178fbb28)
 
其中S是数据的协方差矩阵，定义如下式所示。
  
![](https://pic.superbed.cn/item/5c83a2cf3a213b04178fbda8)

现在为了最大化投影方差，需要寻找一个最佳的方向。这属于一个有约束的最优化问题，约束条件是。为了强制满足这个约束，引入拉格朗日乘数，然后对上式进行无限制最大化。  

![](https://pic.superbed.cn/item/5c83a3013a213b04178fc02a) 
 
通过求取上式的关于的导数值为零，可知驻点满足公式。  

![](https://pic.superbed.cn/item/5c83a3263a213b04178fc164)
  
这表示是协方差矩阵S的一个特征向量，将上式右乘，得到式。
  
![](https://pic.superbed.cn/item/5c83a3703a213b04178fc44a)
  
倘若将上式推广到M维，这便是协方差矩阵的谱分解形式，也叫做特征值分解。这表明，协方差矩阵S可以通过谱分解，得到特征值最大的几个方向。
由于协方差矩阵是对称矩阵，则其必然是正规矩阵，对任意一个正规矩阵而言，都可以将其进行谱分解为式。  

![](https://pic.superbed.cn/item/5c83a39c3a213b04178fc975)
  
其中S为正规的协方差矩阵，U为酉矩阵，为协方差矩阵的特征值。取M<N个特征值，其对应的特征向量就是投影的方向。则在对数据降维使用PCA时，便可以通过先求取数据的协方差矩阵，对协方差矩阵进行谱分解寻找最佳投影方向。

###PCA中的奇异值分解

奇异值分解（SVD）也是对矩阵进行分解，但是和谱分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m x n的矩阵，那么我们定义矩阵A的SVD如式所示。  

![](https://pic.superbed.cn/item/5c83a3e33a213b04178fcc58)
  
其中U是一个m x m的矩阵，Σ是一个m x n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个n x n的矩阵。U和V都是酉矩阵。  

对于m x n的矩阵A，在进行PCA降维之前，还会经常对其进行均值化处理，使其均值为零。如式所示。  

![](https://pic.superbed.cn/item/5c83a4223a213b04178fcedc)
  
其中a是均值，计算公式如式所示。  

![](https://pic.superbed.cn/item/5c83a4323a213b04178fcf9a) 
 
当一个矩阵进行均值化处理之后，求其协方差，可以发现它的协方差矩阵如式所示。  

![](https://pic.superbed.cn/item/5c83a43b3a213b04178fd021) 
 
可以发现，对此协方差矩阵C求谱分解等价于对矩阵求奇异值分解。的奇异值分解如式所示。  

![](https://pic.superbed.cn/item/5c83a4463a213b04178fd0a9) 
 
而通过的奇异值得到的投影方向如式所示。  

![](https://pic.superbed.cn/item/5c83a4523a213b04178fd149)
  
对进行奇异值分解，也可以找到使方差最大的投影方向，即是前M个奇异值最大的方向。

##总结
谱分解和奇异值分解在机器学习算法中有着广泛的应用，例如谱分解可以用于图谱分析和主成分分析，SVD可以用于数据压缩和去噪，也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。  

而当数据经过均值化处理之后，谱分析和奇异值分析的内在联系又特别大，例如在PCA降维中，对目标数据可以先通过求协方差矩阵，再进行谱分解得到投影方向，也可以对目标数据进行均值化处理，使均值为零，再进行奇异值分解得到投影方向。

##参考文献
概率统计简明教程 中国石油大学出版社 卓相来

应用随机过程 电子工业出版社 李晓峰

Pattern Recognition and Machine Learning. Christopher M.Bishop

赵学智,叶邦彦,陈统坚.矩阵构造对奇异值分解信号处理效果的影响[J].华南理工大学学报(自然科学版),2008(09):86-93.

<center>
by [fuweifu](http://www.vtoo.pro)  
<center>
<center>
<img src="https://pic.superbed.cn/item/5c83a7243a213b04178fed8c" width="10%" height="10%" />
</center>
