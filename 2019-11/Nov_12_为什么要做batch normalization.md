Nov_12_为什么要做batch normalization以及一些BN的小细节
====

1. 机器学习领域有个很重要的假设：独立同分布假设;在数据预处理阶段我们一般会做“白化”操作，一个是去除特征之间的相关性;另一个是使得所有特征具有相同的均值和方差，也即保持数据的同分布。

2. BN是用来解决“Internal Covariate Shift(内部协变量偏移)”问题的.Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层;

3. "domain 自适应"是为了解决 external covariate shift,而BN为了解决Internal Covariate Shift.

4. BN的基本思想其实相当直观：能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了ICS问题了。

5. BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。(从 分布角度分析 BN加快训练 和 防止梯度消失 的原因)

6. BN应作用在非线性映射前，即对 x = Wu+b 做规范化.

6. 为什么要做BN
	- 主要是解决梯度消失(梯度弥散)问题,关于梯度弥散，大家都知道一个简单的栗子:0.9**30 = 0.04
	- 主要是解决 internal covariance shift 问题,将每一层的输入强行拉倒均值为0,方差为1的正态分布,这样对非线性激活函数而言可以放大微小变化带来的梯度

7. 如何做BN:![](./images/BN1.jpg)要学习的参数看图所示.

8. 暂时理解为:batch normalization 的参数是一个通道两个,即输入feature map为6,则参数为6个贝塔,6个gamma.

9. batch normalization的另外两个作用:
	- 加速收敛(加快训练速度):原因上面
	- 防止梯度消失